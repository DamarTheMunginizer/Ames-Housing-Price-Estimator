


























# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns    

from sklearn.linear_model import LinearRegression, Lasso
from sklearn.model_selection import train_test_split
from sklearn.dummy import DummyRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

import statsmodels.api as sm


# Read-In datasets (Training and Test(kaggle submission))
df = pd.read_csv('../datasets/train.csv')
df_t = pd.read_csv('../datasets/test.csv')











# View the data size
df.shape


# View the training data
df.head()


# Apply snake_case
df.columns = df.columns.str.replace(' ', '_').str.lower()


# View data with snake case applied
df.head()


# Check for missing values in the columns
df.isnull().sum()


# Completing quick EDA to get a starting model
df_num = df.select_dtypes(include=['float64', 'int64'])

df_num = df_num.fillna(df_num.mean())


# Create a model to test all numeric values
# Assign variables
X = df_num.drop(columns=['saleprice'])
y = df_num['saleprice']


# Train, Test, Split and establishing the model and fitting it 
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)
lr = LinearRegression()
lr.fit(X_train,y_train)

# Evaluate the model on training data
lr.score(X_train, y_train)


# Evaluate on Test data
lr.score(X_test, y_test)


# Baseline Model Comparison
dummy = DummyRegressor(strategy='mean')
dummy.fit(X_train, y_train)
y_dummy_pred = dummy.predict(X_test)
print("Baseline R^2 Score:", r2_score(y_test, y_dummy_pred))


# Baseline Model Comparison
dummy = DummyRegressor(strategy='median')
dummy.fit(X_train, y_train)
y_dummy_pred = dummy.predict(X_test)
print("Baseline R^2 Score:", r2_score(y_test, y_dummy_pred))








# Test Data Run
df_t.head()


# Apply snake_case
df_t.columns = df_t.columns.str.replace(' ', '_').str.lower()


# View the data in the test set
df_t.head()


# View the amount of data in the test dataset
df_t.shape


# Completing quick EDA to get a starting model submission
df_numz = df_t.select_dtypes(include=['float64', 'int64'])

df_numz = df_numz.fillna(df_numz.median())


# 
df_t['saleprice'] = lr.predict(df_numz)


df_t.head()


# Rename the columns to align with kaggle requirements 
df_t0= df_t.rename(columns= {'saleprice': 'SalePrice', 'id': 'ID'})


# Save only the ID and Saleprice in the dataset
test_preds = df_t0[['ID', 'SalePrice']]


# Set the ID as the index
test_preds.set_index('ID', inplace=True)


# View the data ensuring it aligns with kaggle requirement setup
test_preds.head()


# Save the submission for submission
test_preds.to_csv('bl_submission.csv')





# View all columns that are not numeric
objects = df.select_dtypes(include=['object']).columns
df[objects].head()


# View the objects in the dataset
objects.unique()





# View the street column choices
df['street'].unique()


# View the central air column choices
df['central_air'].unique()





# Binarize the columns
df['street_binarized'] = df['street'].map({'Grvl': 0, 'Pave': 1})
df['central_air_binarized'] = df['central_air'].map({'Y': 1, 'N':0})


# View the columns have been added
df.head()


# Check to ensure the added columns are now numeric
df.dtypes


# Quick EDA with binarized columns and fill in missing values
df_num = df.select_dtypes(include=['float64', 'int64'])
df_num = df_num.fillna(df_num.median())

df_num.head()


# Create a model to test all numeric values
# Assign variables
X = df_num.drop(columns=['saleprice'])
y = df_num['saleprice']


# Conduct a Train, Test, Split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)

# Instaniate the model
lr = LinearRegression()

# Fit the model 
lr.fit(X_train,y_train)

# Check the accuracy of the model on training data
lr.score(X_train, y_train)


# Check the accuracy of the model on the testing data
lr.score(X_test, y_test)





# Model on Test data for kaggle submission
df_t.head()


# Binarize the testing data columns
df_t['street_binarized'] = df_t['street'].map({'Grvl': 0, 'Pave': 1})
df_t['central_air_binarized'] = df_t['central_air'].map({'Y': 1, 'N':0})


# Quick EDA with binarized columns aand fill in the misisng values
df_numz2 = df_t.select_dtypes(include=['float64', 'int64'])
df_numz2 = df_numz2.fillna(df_numz2.median())

df_numz2.head()


# Assign the predictions as the saleprice column
df_t['saleprice'] = lr.predict(df_numz2.drop(columns=['saleprice']))


# View the data to ensure the predictions where added 
df_t.head()


# Rename the columns to align with kaggle requirements 
df_t1 = df_t.rename(columns= {'saleprice': 'SalePrice', 'id': 'ID'})


# Save only the ID and Saleprice in the dataset
test_predz = df_t1[['ID', 'SalePrice']]


# Set the ID as the index
test_predz.set_index('ID', inplace=True)


# View the data ensuring it aligns with kaggle requirement setup
test_predz.head()


# Save the submission for submission
test_predz.to_csv('86.04%_submission.csv')








# Drop non-numeric columns or columns that cannot be used for correlation calculation
     # df = df.select_dtypes(include=['float64', 'int64'])

# Handle missing values (you can fill with mean or drop rows/columns as needed)
    # df = df.fillna(df.median())

# Already completed this in the quick EDA but will redo it for a reminder.


corr_matrix = df_num.corr()

# Create a heatmap.
plt.figure(figsize=(3, 15))
sns.heatmap(df_num.corr()[['saleprice']].sort_values(by='saleprice',ascending = False), 
            annot=True,
            cmap='magma');



# View the columns in the heatmap
numerics = df_num.select_dtypes(include=['float64', 'int64']).columns
df_num[numerics].head()


# Compute the correlation matrix
corr_saleprice = corr_matrix['saleprice'].abs()
# Apply a filter so that we only view those that have an impact on the salesprice of 20% or higher
filtered_columns = corr_saleprice[corr_saleprice >= 0.2].index.tolist()


# Create a filter correlation matrix
filtered_matrix = corr_matrix[filtered_columns].loc[filtered_columns]


# Plot the heatmap
plt.figure(figsize=(3, 15))
sns.heatmap(filtered_matrix[['saleprice']].sort_values(by='saleprice',ascending = False), 
            annot=True,
            cmap='magma');
# Still hard to read so make more adjsutments for readibility


# View the rows in the heatmap
df_filt = df_num[filtered_columns]
df_filt.head()





# Create a model to test all numeric values
# Assign variables
X = df_filt.drop(columns=['saleprice'])
y = df_filt['saleprice']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)
lr = LinearRegression()
lr.fit(X_train,y_train)

lr.score(X_train, y_train)


lr.score(X_test, y_test)








# Create a mask for missing values (NaN) and zeros
missing = df_filt.isnull()  # Mask for NaN values
zero = (df_filt == 0)  # Mask for zero values

# Combine both masks
combined_mask = missing | zero

# Calculate the proportion of missing or zero values in each row
proportion = combined_mask.mean(axis=1)

# Identify rows where the proportion is greater than or equal to 50%
rows_to_drop = proportion >= 0.5

# Identify columns with `0` or missing values in these rows
columns_to_drop = combined_mask.loc[rows_to_drop, :].sum(axis=0)

# Get the column names where more than half of the rows have `0` or missing values
columns_drop = columns_to_drop[columns_to_drop > 0].index.tolist()

# Display the columns
print(columns_drop)


#Drop the columns with >= 50% of the data missing
df_filt = df_filt.drop(columns=columns_drop)


#Calulate the correlatiom matrix
corr_matrix = df_filt.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Heatmap (Excluding Columns with > 50% Missing or Zero Values)')
plt.show()


# View the filtered columns
df_filt.head()


# Label the variables
X = df_filt.drop(columns=['saleprice'])
y = df_filt['saleprice']


# Split The Data into TTS
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Instantiate the Model and Fit
lr = LinearRegression()
lr.fit(X_train,y_train)

lr.score(X_train, y_train)


lr.score(X_test, y_test)








# Training Data
df.head()


# Testing Data
df_t.head()


# View only numeric values

# df_num = df.select_dtypes(include=['float64', 'int64'])
# df_num = df_num.fillna(df_num.median())

# View the categorical values
df_cat = df.drop(columns=df_num)


# Fill in the missing data
df.fillna(df_num.median(), inplace=True)
df.fillna(df_cat.mode().iloc[0], inplace=True)


# Find the highest year so we can give the house an age at sale
max_year = df_num['yr_sold'].max()
max_year


# Assign the house an age
df_num['age'] = 2010 - df_num['year_built']


# View the difference to ensure the age is correct
df_num[['year_built', 'age']].head()


# Log transformation of the target variable (saleprice)
df_num['log_saleprice'] = np.log1p(df_num['saleprice'])

# Visualize the distribution before and after transformation
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.histplot(df_num['saleprice'], kde=True)
plt.title('Original SalePrice Distribution')

plt.subplot(1, 2, 2)
sns.histplot(df_num['log_saleprice'], kde=True)
plt.title('Log-Transformed SalePrice Distribution')

plt.tight_layout()
plt.show()


# View the data in the numeric variable
df_num.head()


corr_matrix = df_num.corr()

# Create a heatmap.
plt.figure(figsize=(3, 15))
sns.heatmap(df_num.corr()[['saleprice']].sort_values(by='saleprice',ascending = False), 
            annot=True,
            cmap='magma');


# Create a function that calls the features that make up at least 25% of the saleprice.
def select_high_correlation_features(df, target, threshold=0.25):
    
    # Calculate the correlation matrix
    correlation_matrix = df_num.corr()

    # Get the correlation of each feature with the target variable
    target_corr = correlation_matrix[target]

    # Select features with correlation greater than or equal to the threshold
    selected_features = target_corr[target_corr.abs() >= threshold].index.tolist()

    # Remove the target variable itself from the selected features list
    selected_features.remove(target)

    return selected_features


# Assign a varable to the features
high_corr_features = select_high_correlation_features(df, 'saleprice', threshold=0.25)


# View the features
df_num[high_corr_features]


# Assign the variables for model fitting
X = df_num[high_corr_features].drop(columns='log_saleprice')
y = df_num['log_saleprice']


# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Initialize the Linear Regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(X_train, y_train)

# Check the model's coefficients (features importance)
coefficients = pd.DataFrame(model.coef_, X.columns, columns=['Coefficient'])
print(coefficients)





# Remove the negative coeffients from the features
high_corr_features.remove('pid')
high_corr_features.remove('mas_vnr_area')
high_corr_features.remove('half_bath') 
high_corr_features.remove('garage_yr_blt')
high_corr_features.remove('open_porch_sf')
high_corr_features.remove('age')


# Assign the varibles to fit the model with the (-) coefficients removed
X = df_num[high_corr_features].drop(columns='log_saleprice')
y = df_num['log_saleprice']


# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Linear Regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(X_train, y_train)


# Make predictions on the test data from the split
y_pred = model.predict(X_test)

# Convert log-transformed predictions back to the original scale
y_pred_original = np.expm1(y_pred)
y_test_original = np.expm1(y_test)


# Calculate MSE
mse = mean_squared_error(y_test_original, y_pred_original)
print(f"Mean Squared Error (MSE): {mse}")

# Calculate MAE
mae = mean_absolute_error(y_test_original, y_pred_original)
print(f"Mean Absolute Error (MAE): {mae}")


# Calculate (R-squared)
r2 = r2_score(y_test_original, y_pred_original)
r2





# Plot actual vs predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test_original, y_pred_original)
plt.plot([0, max(y_test_original)], [0, max(y_test_original)], color='red', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Sale Prices')
plt.show()





# Showing whats under the variables
#df_numz = df_t.select_dtypes(include=['float64', 'int64'])

#df_numz = df_numz.fillna(df_numz.median())


# Handle the missing values in categorical columns
df_cate = df_numz.drop(columns = df_numz)


# Remove the log_price as we will not have it on the test data becasue we are looking to predict it.
high_corr_features.remove('log_saleprice')


# Process the test data just like the training data
# Handle missing values for numeric and categorical columns in the test set
df_t.fillna(df_numz.median(), inplace=True)
df_t.fillna(df_cate.mode(), inplace=True)

# Select the features for the model (same as in training)
X_test = df_t[high_corr_features]  # features for testing data

# Predict using the trained model
y_pred_log = model.predict(X_test)

# Reverse any transformations (e.g., log transformation)
y_pred = np.expm1(y_pred_log)  # Convert from log scale back to original scale

# Prepare submission
submission = pd.DataFrame({'ID': df_t['id'], 'SalePrice': y_pred})


# View the predictions from the model
submission.head()


# Set the ID as the index for the kaggle submission rules and verify the changes
submission.set_index('ID', inplace=True)
submission.head()


#Submit to kaggle
submission.to_csv('89%_submission.csv')






